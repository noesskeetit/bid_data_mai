# Readme

## Описание проекта

Данный проект реализован в соответствии с алгоритмом выполнения задания. Все основные требования выполнены:

1. Код разработан с использованием Apache Spark и PostgreSQL.
2. PostgreSQL и Spark запускаются в контейнерах через Docker.
3. Инструкция по запуску включена в данный файл.
4. Реализована инкрементальная загрузка данных.

### Особенности реализации

- Таблицы в PostgreSQL создаются автоматически.
- Загрузка данных и настройка окружения полностью автоматизированы и выполняются через `docker-compose.yml` файл.

## Как запустить проект

1. Склонируйте репозиторий к себе:
   ```bash
   git clone <URL_репозитория>
   cd <имя_папки_с_проектом>
   ```

2. Откройте терминал, находясь в папке с проектом, и выполните команду:
   ```bash
   docker-compose up --force-recreate
   ```

3. Подождите, пока все сервисы полностью поднимутся. На это может потребоваться несколько минут.

4. Перейдите на сервер со Spark через веб-интерфейс по адресу отображенном в терминале, к примеру:
   ```
   http://localhost:8080
   ```

5. Откройте скрипт `ETL_PIPELINE.ipynb` и выполните все ячейки. Убедитесь, что:
   - Инкрементальная загрузка данных выполняется корректно.
   - Результаты обработки данных соответствуют требованиям.

## Структура проекта

- **Docker-compose.yml**: файл для автоматического развертывания сервисов PostgreSQL и Spark.
- **ETL_PIPELINE**: скрипт с реализацией ETL процесса и инкрементальной загрузки данных.
- **README.md**: текущий файл с описанием процесса.

## Примечания

- Все действия протестированы и работают в стандартной среде Docker.
- При возникновении вопросов обращайтесь в Telegram к @xxxbibizion.

